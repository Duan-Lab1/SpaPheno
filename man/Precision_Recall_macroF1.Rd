% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Precision_Recall_macroF1.R
\name{Precision_Recall_macroF1}
\alias{Precision_Recall_macroF1}
\title{Compute Macro-Averaged Precision, Recall, and F1 Score for Multiclass Classification}
\usage{
Precision_Recall_macroF1(actual, predicted)
}
\arguments{
\item{actual}{A vector of true class labels.}

\item{predicted}{A vector of predicted class labels (must have same length as \code{actual}).}
}
\value{
A named list containing:
\itemize{
  \item \code{Precision}: Macro-averaged precision across all classes.
  \item \code{Recall}: Macro-averaged recall across all classes.
  \item \code{f1_score}: Macro-averaged F1 score.
}
}
\description{
Computes macro-averaged precision, recall, and F1 score using a confusion matrix
between true labels and predicted labels. Suitable for multi-class classification tasks.
}
\details{
Macro-averaging computes the mean of the metric (precision, recall, F1) over all classes,
treating all classes equally regardless of their support. Missing values due to undefined metrics
are excluded from the averaging (e.g., divisions by zero).
}
\examples{
\dontrun{
actual <- c("A", "B", "A", "C", "B", "C")
predicted <- c("A", "B", "C", "C", "B", "A")
Precision_Recall_macroF1(actual, predicted)
}

}
\author{
Bin Duan
}
